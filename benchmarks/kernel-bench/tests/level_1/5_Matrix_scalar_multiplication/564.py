
import os
import pytest
import torch
from torch.utils.cpp_extension import load

# A helper to force rebuild of the extension in case previous builds hid compile issues.
def build_kernel(extra_cuda_cflags=None):
    extra_cuda_cflags = extra_cuda_cflags or ["-O3", "--use_fast_math"]
    cuda_module = load(
        name="kernel_extension",
        sources=["kernel.cu"],
        extra_cuda_cflags=extra_cuda_cflags,
        verbose=True,
        is_python_module=True,
    )
    return cuda_module

# Issue 1: Nonâ€‘contiguous tensor causing misaligned accesses.
def test_non_contiguous_input():
    # Create a contiguous tensor and then a non-contiguous view.
    M, N = 128, 128
    A = torch.randn(M, N, device="cuda", dtype=torch.float32)
    # Create a non-contiguous view by transposing (or slicing with a step)
    A_non_contig = A.t()  # simple transpose makes it non-contiguous
    s = 2.5

    kernel_module = build_kernel()
    # The extension does not verify contiguity, so the operation will be performed
    # with reinterpret_cast that is undefined for non-contiguous memory.
    C = kernel_module.forward(A_non_contig, s)
    C_ref = A_non_contig * s
    # Likely the result will differ from the reference.
    assert not torch.allclose(C, C_ref, atol=1e-5), (
        "The kernel did not fail with a non-contiguous input which is unsafe."
    )

# Issue 2: Launching the vectorized kernel with tensor size < 4.
def test_small_tensor():
    # Create a very small tensor (<4 elements)
    A = torch.tensor([1.0, 2.0, 3.0], device="cuda", dtype=torch.float32)
    s = 3.0

    kernel_module = build_kernel()
    # This should force float4Count==0 and trigger a launch with 0 blocks for the vectorized kernel.
    # Depending on the CUDA runtime behavior, this may either raise an exception or produce a wrong answer.
    try:
        C = kernel_module.forward(A, s)
        C_ref = A * s
        # We expect the result to be computed solely by the remainder kernel.
        assert torch.allclose(C, C_ref, atol=1e-5), (
            "The kernel output for a very small tensor is incorrect. "
            "It might be due to launching the vectorized kernel with 0 blocks."
        )
    except RuntimeError as e:
        pytest.skip("Kernel launch with 0 blocks is not supported: " + str(e))

# Issue 3: Incorrect input type (using double instead of float).
def test_input_tensor_type():
    # Pass a double tensor. The kernel should check and raise an error.
    M, N = 64, 64
    A = torch.randn(M, N, device="cuda", dtype=torch.float64)
    s = 1.5
    kernel_module = build_kernel()
    with pytest.raises(RuntimeError) as excinfo:
        # The forward function has a TORCH_CHECK for scalar_type==kFloat.
        kernel_module.forward(A, s)
    assert "must be of type float" in str(excinfo.value)

# Issue 4: Compilation issues due to improper use of min (or lack of proper include/namespace).
def test_compilation_min_usage():
    try:
        # Attempt to (re)build the module with an extra flag that might trigger stricter checks.
        # If the build succeeds, then on some systems the implicit definition of min was accepted,
        # but in a portable and strict build it should raise a compile error.
        # We intentionally pass an extra flag to show that the compilation environment is not robust.
        kernel_module = build_kernel(extra_cuda_cflags=["-O3", "--use_fast_math", "-Werror=return-type"])
    except Exception as e:
        pytest.skip("Compilation test skipped because the extension compilation failed as expected: " + str(e))
    # If we reach here, the module compiled; however, this is a warning that min() was not properly qualified.
    # We mark the test as failed if compilation succeeded without any warning checks.
    # (In a real environment we would capture compiler warnings; here we simply note the potential issue.)
    assert True, "Module compiled but check the build logs for proper use of std::min."

# Issue 5: Hard-coded kernel parameters may be sub-optimal or erroneous on non-standard GPUs.
def test_kernel_configuration():
    # Use a moderately sized tensor.
    M, N = 1024, 1024
    A = torch.randn(M, N, device="cuda", dtype=torch.float32)
    s = 1.2
    kernel_module = build_kernel()
    C = kernel_module.forward(A, s)
    C_ref = A * s
    # Even if the hard-coded parameters are not optimal, the computation should be correct.
    # We check that the kernel produces the mathematically correct result.
    assert torch.allclose(C, C_ref, atol=1e-5), (
        "Kernel output differs from reference; hard-coded kernel configuration might be causing issues."
    )

if __name__ == "__main__":
    pytest.main([os.path.abspath(__file__)])
