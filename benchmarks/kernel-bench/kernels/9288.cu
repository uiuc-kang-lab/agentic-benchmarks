__global__ void optimized_conv_transpose1d_kernel(const float* __restrict__ x_ptr, const float* __restrict__ weight_ptr, const float* __restrict__ bias_ptr, float* __restrict__ output_ptr, int batch_size, int in_channels, int out_channels, int input_length, int output_length, int kernel_size, int stride, int padding, int dilation) { extern __shared__ float shared_mem[]; float* shared_weight = shared_mem; float* partial_sums = &shared_mem[in_channels * kernel_size]; const int tid = threadIdx.x; const int out_idx = blockIdx.x; if (out_idx >= batch_size * out_channels * output_length) return; const int b = out_idx / (out_channels * output_length); const int rem = out_idx % (out_channels * output_length); const int oc = rem / output_length; const int o = rem % output_length; const int weight_offset = oc * in_channels * kernel_size; for (int i = tid; i < in_channels * kernel_size; i += blockDim.x) { shared_weight[i] = weight_ptr[weight_offset + i]; } __syncthreads(); float sum = 0.0f; for (int idx = tid; idx < in_channels * kernel_size; idx += blockDim.x) { const int k = idx / in_channels; const int ic = idx % in_channels; const int i_pos = o * stride + padding - k * dilation; if (i_pos % stride == 0) { const int i = i_pos / stride; if (i >= 0 && i < input_length) { const int x_idx = b * in_channels * input_length + ic * input_length + i; sum += x_ptr[x_idx] * shared_weight[ic * kernel_size + k]; } } } partial_sums[tid] = sum; __syncthreads(); for (int s = blockDim.x/2; s > 0; s >>= 1) { if (tid < s) { partial_sums[tid] += partial_sums[tid + s]; } __syncthreads(); } if (tid == 0) { float final_sum = partial_sums[0]; if (bias_ptr) { final_sum += bias_ptr[oc]; } output_ptr[out_idx] = final_sum; }}