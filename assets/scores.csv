Benchmark,Outcome Validity,Task Validity,Benchmark Reporting,Overall
[MLE-Bench](https://github.com/openai/mle-bench),100.0,90.0,92.3,94.1
[Cy-Bench](https://cybench.github.io/),100.0,100.0,69.2,89.7
[GAIA](https://huggingface.co/spaces/gaia-benchmark/leaderboard),100.0,60.0,53.8,71.3
[$\\tau$-Bench](https://sierra.ai/blog/benchmarking-ai-agents/),50.0,100.0,46.2,65.4
[OSWorld](https://os-world.github.io/),66.7,80.0,46.2,64.3
[SWE-Bench-Lancer](https://github.com/openai/SWELancer-Benchmark/),50.0,80.0,53.8,61.3
[SWE-Bench-Verified](https://openai.com/index/introducing-swe-bench-verified/),50.0,100.0,30.8,60.3
[Bird-Bench](https://bird-bench.github.io/),50.0,60.0,46.2,52.1
[WebArena](https://webarena.dev/),50.0,40.0,46.2,45.4
[Kernel-Bench](https://scalingintelligence.stanford.edu/blogs/kernelbench/),0.0,80.0,53.8,44.6
