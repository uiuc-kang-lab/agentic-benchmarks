Benchmark,Outcome Validity,Task Validity,Benchmark Reporting,Overall,Contributor
[MLE-Bench](https://github.com/openai/mle-bench),100.0,90.0,92.3,94.1,AgenticBenchmarkChecklist Team
[CyBench](https://cybench.github.io/),100.0,100.0,69.2,89.7,AgenticBenchmarkChecklist Team
[GAIA](https://huggingface.co/spaces/gaia-benchmark/leaderboard),100.0,60.0,53.8,71.3,AgenticBenchmarkChecklist Team
[$\\tau$-Bench](https://sierra.ai/blog/benchmarking-ai-agents/),50.0,100.0,46.2,65.4,AgenticBenchmarkChecklist Team
[OSWorld](https://os-world.github.io/),66.7,80.0,46.2,64.3,AgenticBenchmarkChecklist Team
[SWE-Bench-Lancer](https://github.com/openai/SWELancer-Benchmark/),50.0,80.0,53.8,61.3,AgenticBenchmarkChecklist Team
[SWE-Bench-Verified](https://openai.com/index/introducing-swe-bench-verified/),50.0,100.0,30.8,60.3,AgenticBenchmarkChecklist Team
[Bird-Bench](https://bird-bench.github.io/),50.0,60.0,46.2,52.1,AgenticBenchmarkChecklist Team
[WebArena](https://webarena.dev/),50.0,40.0,46.2,45.4,AgenticBenchmarkChecklist Team
[KernelBench](https://scalingintelligence.stanford.edu/blogs/kernelbench/),0.0,80.0,53.8,44.6,AgenticBenchmarkChecklist Team
