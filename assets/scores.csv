Benchmark,Outcome Validity,Challenge Validity,Benchmark Reporting,Overall
[GAIA](https://huggingface.co/spaces/gaia-benchmark/leaderboard),100.0,100.0,90.0,96.7
[MLE-Bench](https://github.com/openai/mle-bench),100.0,88.9,70.0,86.3
[Cy-Bench](https://cybench.github.io/),100.0,100.0,40.0,80.0
[$\\tau$-Bench](https://sierra.ai/blog/benchmarking-ai-agents/),50.0,100.0,30.0,60.0
[Bird-Bench](https://bird-bench.github.io/),50.0,77.8,50.0,59.3
[SWE-Bench-Verified](https://openai.com/index/introducing-swe-bench-verified/),25.0,100.0,40.0,55.0
[Kernel-Bench](https://scalingintelligence.stanford.edu/blogs/kernelbench/),16.7,77.8,55.0,49.8
[Math500](https://github.com/openai/prm800k/tree/main?tab=readme-ov-file#math-splits),25.0,88.9,30.0,48.0
[SWE-Bench-Lancer](https://github.com/openai/SWELancer-Benchmark/),33.3,66.7,40.0,46.7
[WebArena](https://webarena.dev/),37.5,66.7,30.0,44.7
