paper: https://arxiv.org/pdf/2307.13854
code: https://github.com/web-arena-x/webarena

O.a.1:
  check: Considers expressions semantically equivalent to ground truth.
  score: 1
  reason: As discussed in Section 3.2 of the paper, the benchmark expects the response to follow a standardized format, which is robust to variations in the input.

O.a.2:
  check: Handles redundant words used by agents.
  score: 1
  reason: As discussed in Section 3.2 of the paper, the benchmark expects the response to follow a standardized format, which is robust to redundant words in the input.

O.b.1:
  check: Handles negation modifiers used by agents.
  score: 0
  reason: The benchmark does not handel negation modifiers, which may lead to incorrect evaluations.

O.b.2:
  check: Is robust against systematically listing all possible answers.
  score: 0
  reason: The benchmark does not specify how it handles systematic listing of all possible answers, which may lead to incorrect evaluations.

O.b.3:
  check: Ground truth is sufficiently complex to prevent guessing.
  score: 0
  reason: The ground truth is NULL for a part of tasks, which may lead to guessing.

O.c.1:
  check: Demonstrates documented or experimental evidence of the judgeâ€™s accuracy, self-consistency, and agreement with human.
  score: 1
  reason: The accuracy of the judge is quantitatively evaluated in Appendix A.8 of the paper.

O.c.2:
  check: Is designed to resist adversarial inputs and reward hacking.
  score: 0
  reason: The benchmark does not handel adversarial inputs and reward hacking in LLM-as-a-Judge, which may lead to incorrect evaluations.

O.g.1:
  check: Ground truth includes all states achievable after success.
  score: 1
  reason: The ground truth includes all states achievable after success, as discussed in Section 3.2 of the paper.

O.g.2:
  check: Checks relevant and irrelevant states for the tasks.
  score: 0
  reason: The state check only considers relevant states (e.g., acheived by using a locator as discussed in Section 3.2), which may lead to incorrect evaluations.

O.g.3:
  check: Ground truth is complex to prevent trivial state modifications.
  score: 1
  reason: As demonstrated in Section 3.2 of the paper, the ground truth is a modification of the underlying database, which is complex enough to prevent trivial state modifications.

T.1:
  check: Versions of all tools (e.g., Python) are clearly specified.
  score: 1
  reason: The benchmark does not use tools that require version specification.

T.2:
  check: All required API tools are consistently accessible to agents during evaluation.
  score: 0
  reason: The benchmark requires an external API (e.g., a clone of Reddit website) that is not can be inaccessible to agents during evaluation due to rate limit.

T.3:
  check: Evaluation process terminates or handles errors appropriately if an API becomes inaccessible.
  score: 0
  reason: The evaluation process does not handle errors appropriately if the API becomes inaccessible, which may lead to incorrect evaluations.

T.4:
  check: Residual data or state are fully cleared between runs.
  score: 1
  reason: The benchmark uses docker containers to isolate the environment, and the state is cleared between runs.

T.5:
  check: Agent is completely isolated from any ground truth information.
  score: 1
  reason: The agent has no access to the file system where the ground truth is stored.

T.6:
  check: Setup does not change over time (e.g., no live website).
  score: 1
  reason: The environment setup is static and does not change over time.

T.7:
  check: Annotated ground truth is verified for correctness.
  score: 0
  reason: As mentioned in Section 3.2, the ground truth is annotated by two human annotators. However, there isn't a mechanism to verify or guarantee the correctness of the annotations.

T.8:
  check: Each task is verified to be solvable.
  score: 0
  reason: The ambiguity of the tasks is not fully verified or tested, which may lead to incorrect evaluations.

T.9:
  check: Benchmark includes an Oracle solver that can automatically solve all tasks.
  score: 0
  reason: The benchmark does not include an Oracle solver that can automatically solve all tasks.

T.10:
  check: Implementation is free of vulnerabilities that could be exploited to pass evaluations without completing tasks.
  score: 0
  reason: A do-nothing agent can pass 4.4% of the tasks. These tasks use N/A as the ground truth.

R.1:
  check: Is fully or at least partially open-sourced.
  score: 1
  reason: The benchmark is open-sourced and available on GitHub.

R.2:
  check: Offers an open-source evaluation harness for users.
  score: 1
  reason: The benchmark provides an open-source evaluation harness for users.

R.3:
  check: Includes measures to prevent data contamination, such as a private, held-out test set.
  score: 0
  reason: The benchmark does not discuss measures to prevent data contamination.

R.4:
  check: Includes measures or plans to consistently update tasks over time to avoid overfitting.
  score: 0
  reason: The benchmark does not discuss plans to consistently update tasks over time.

R.5:
  check: Clearly states the relationship between the agent capabilities it aims to evaluate and the constructs or outcomes it measures.
  score: 1
  reason: Such a relationship is clearly stated in Section 2.1 of the paper.

R.6:
  check: Clearly states the evaluation subjective of the benchmark (e.g., a model or an agent framework).
  score: 1
  reason: As shownin Setion 5, the benchmark is designed to evaluate LLM models.

R.7:
  check: Describes steps taken to prevent, identify, and correct flaws.
  score: 1
  reason: Efforts to evaluate LLM-as-a-Judge are discussed in Appendix A.8 of the paper.

R.8:
  check: Includes qualitative discussions of the potential impact of unavoidable flaws.
  score: 0
  reason: The report does not discuss the potential impact of unavoidable flaws.

R.9:
  check: Includes quantitative analysis to assess the impact of unavoidable flaws (e.g., noise of ground truth).
  score: 0
  reason: The report does not include quantitative analysis to assess the impact of unavoidable flaws.

R.10:
  check: Reports metrics about statistical significance, such as confidence intervals.
  score: 0
  reason: The report does not include any metrics about statistical significance.

R.11:
  check: Provides guidance on interpreting results with eval flaws.
  score: 0
  reason: The report does not provide any guidance on interpreting results with eval flaws.

R.12:
  check: Reports results of non-AI baselines (e.g., human experts).
  score: 1
  reason: The human performance is reported in appendix A.5.

R.13:
  check: Reports results of trivial agents (e.g., one that does nothing).
  score: 0
  reason: The report does not report results of trivial agents.
