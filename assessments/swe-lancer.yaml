paper: https://arxiv.org/pdf/2502.12115
code: https://github.com/openai/SWELancer-Benchmark?tab=readme-ov-file

O.d.1:
  check: Verifies test cases for correctness and quality (e.g., by human).
  score: 1
  reason: As discussed in Section 1 of the paper, the benchmark uses a set of test cases that are verified for correctness and quality by human experts.

O.d.2:
  check: Measures quality of test cases using objective metrics (e.g., code coverage, cyclomatic complexity control).
  score: 0
  reason: The benchmark does not use objective metrics to measure the quality of test cases.

O.f.2:
  check: Exercises all relevant parts of the code being tested.
  score: 1
  reason: As discussed in Section 1, the end-to-end testing is designed to simulate the entire user workflow.

O.f.3:
  check: Prevents non-deterministic (“flaky”) test results.
  score: 0
  reason: The test cases use hard-coded timeouts, which may lead to non-deterministic results if the system is slow or unresponsive.

T.1:
  check: Versions of all tools (e.g., Python) are clearly specified.
  score: 1
  reason: The package dependencies are specified in the repository of each task.

T.2:
  check: All required API tools are consistently accessible to agents during evaluation.
  score: 1
  reason: The benchmark does not require any external APIs.

T.3:
  check: Evaluation process terminates or handles errors appropriately if an API becomes inaccessible.
  score: 1
  reason: The benchmark does not require any external APIs.

T.4:
  check: Residual data or state are fully cleared between runs.
  score: 1
  reason: The benchmark uses docker containers to isolate the environment, and the state is cleared between runs.

T.5:
  check: Agent is completely isolated from any ground truth information.
  score: 0
  reason: The agent can access the file system where the test cases are stored, which may lead to the agent accessing the ground truth information.

T.6:
  check: Setup does not change over time (e.g., no live website).
  score: 1
  reason: The environment setup is static and does not change over time.

T.7:
  check: Annotated ground truth is verified for correctness.
  score: 1
  reason: The ground-truth test cases are taken from GitHub repositories, which are verified by expert developers.

T.8:
  check: Each task is verified to be solvable.
  score: 1
  reason: Each task represents a real-world software issue with a corresponding patch, which are solvable by the agent.

T.9:
  check: Benchmark includes an Oracle solver that can automatically solve all tasks.
  score: 1
  reason: The benchmark uses existing patches as ground truth, which can be considered as an Oracle solver.

T.10:
  check: Implementation is free of vulnerabilities that could be exploited to pass evaluations without completing tasks.
  score: 0
  reason: The benchmark does not handle the isolation between the agent and test cases properly. The test cases are stored not only in a file system that the agent can access, but also in a ZIP file that agent can read the directory structure and update files.

R.1:
  check: Is fully or at least partially open-sourced.
  score: 1
  reason: The benchmark is open-sourced and available on GitHub.

R.2:
  check: Offers an open-source evaluation harness for users.
  score: 1
  reason: The benchmark provides an open-source evaluation harness for users.

R.3:
  check: Includes measures to prevent data contamination, such as a private, held-out test set.
  score: 1
  reason: The benchmark maintains a private test set.

R.4:
  check: Includes measures or plans to consistently update tasks over time to avoid overfitting.
  score: 0
  reason: The report does not discuss any measures or plans for consistent update.

R.5:
  check: Clearly states the relationship between the agent capabilities it aims to evaluate and the constructs or outcomes it measures.
  score: 1
  reason: Such a relationship is clearly stated in Section 2 of the paper.

R.6:
  check: Clearly states the evaluation subjective of the benchmark (e.g., a model or an agent framework).
  score: 1
  reason: As shown in Section 3, the benchmark is designed to evaluate the LLM model.

R.7:
  check: Describes steps taken to prevent, identify, and correct flaws.
  score: 1
  reason: The benchmark uses end-to-end testing to mitigate grader hacking.

R.8:
  check: Includes qualitative discussions of the potential impact of unavoidable flaws.
  score: 1
  reason: The benchmark discusses the potential impact of grader hacking in Section 1 and Appendix A.7.

R.9:
  check: Includes quantitative analysis to assess the impact of unavoidable flaws (e.g., noise of ground truth).
  score: 0
  reason: The benchmark does not include any quantitative analysis to assess the impact of grader hacking.

R.10:
  check: Reports metrics about statistical significance, such as confidence intervals.
  score: 0
  reason: The benchmark does not report any metrics about statistical significance.

R.11:
  check: Provides guidance on interpreting results with eval flaws.
  score: 0
  reason: The benchmark does not provide any guidance on interpreting results with eval flaws.

R.12:
  check: Reports results of non-AI baselines (e.g., human experts).
  score: 0
  reason: The benchmark does not report results of non-AI baselines.

R.13:
  check: Reports results of trivial agents (e.g., one that does nothing).
  score: 0
  reason: The benchmark does not report results of trivial agents.
