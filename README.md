# Assessing Agentic Benchmarks for Rigorous Evaluation

## Overview

This repository contains a checklist for assessing agentic benchmarks, results 
of assessing existing agentic benchmarks, and experimental scripts for 
reproducing identified issues. 

```bash
agentic-benchmarks/
├── ABC.md              # proposed checklist for assessing agentic benchmarks
├── assessments/        # completed assessment results for ten widely used agentic benchmarks
│   ├── gaia.yaml
│   ├── swe-bench.yaml
│   ├── ....
├── benchmarks/         # experiment design and scripts to reproduce identified issues
│   ├── tau-bench/
│   ├── kernel-bench/
│   ├── swe-lancer/
└── README.md
```

## Agentic Benchmark Checklist (ABC)

ABC is composed of three sections:
1. Outcome Validity: the success signal (e.g., tests or checks) truly indicates 
   that the challenge has been solve
2. Challenge Validity: a challenge should be solvable if and only if the agent 
   possesses the target capability 
3. Benchmark Reporting: when guaranteeing outcome validity and challenge 
   validity is particularly challenging or even impossible, benchmark developers 
   should discuss such issues with quantitative evidence and provide guidelines 
   on interpreting imperfect benchmarking results

We provide all items of ABC in `ABC.md`. 

## Newly Identified Issues

For each identified outcome or challenge validity issues, we demonstrate and
reproduce them with experiments. We present experiments design by either patch
files of original benchmark codebase or an end-to-end pipeline with detailed 
steps.

### 1. Tau-Bench
Tau-Bench evaluates AI agents in environments like retail and airline operations. 
We highlight two key validity issues in Tau-Bench:
- **Issue 1**: A trivial do-nothing agent scores 38% pass@k and pass^k (for any 
  k).
- **Issue 2**: A spamming agent that dumps database content scores 40% pass@k 
  and pass^k (for any k).

#### Reproducing Issues
- **Issue 1**: Apply `exploit-1.patch` and run the trivial do-nothing agent.
- **Issue 2**: Apply `exploit-2.patch` and run the spamming agent.

Refer to [benchmarks/tau-bench/README.md](benchmarks/tau-bench/README.md) for detailed instructions.

---

### 2. Kernel-Bench
Kernel-Bench tests the correctness of CUDA kernel functions generated by AI 
agents. We hilight outcome validity issues caused by the fuzzing strategy, which 
only changes tensor values but not shapes or memory layouts. This issue leads to
an overestimation of agents' capability in writing correct kernel functions by 28%.

#### Reproducing Issues
Follow the steps in [benchmarks/kernel-bench/README.md](benchmarks/kernel-bench/README.md) to reproduce and analyze the issues.

---

### 3. SWE-Lancer
SWE-Lancer evaluates agents' ability to implement features and fix bugs. We 
highlight a challenge validity issue where agents can bypass test cases stored 
in password-protected `.zip` files.

#### Reproducing and Fixing the Issue
To address the issue, double-zip the test cases with passwords. 
See [benchmarks/swe-lancer/README.md](benchmarks/swe-lancer/README.md) for details.

---

## Contributing
Contributions are welcome! If you want to submit new assessment results, request
an assessment correction, or send feedback or question, please submit a pull 
request or open an issue.