# Contribute to the Agentic Benchmark Checklist

Upholding the validity of agentic benchmarks requires effort from the broader scientific community. If you’re passionate about reliable evaluation in AI, we’d love your help.

Here’s some ways to get involved:

1. Apply the checklist to an existing benchmark - submit [here](https://forms.gle/BRrVh8McQaq8tnGc8).

2. Contribute proof-of-concept exploits and fixes for those exploits in our [repo](https://github.com/uiuc-kang-lab/agentic-benchmarks).

3. Give feedback on the checklist itself [here](https://forms.gle/xbGkqVksEH4fTajF8).